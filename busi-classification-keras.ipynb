{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2021025,"sourceType":"datasetVersion","datasetId":1209633},{"sourceId":8012654,"sourceType":"datasetVersion","datasetId":4720478}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import packages","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras import regularizers, layers\n\nfrom collections import defaultdict\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:37:43.370444Z","iopub.execute_input":"2024-04-20T14:37:43.370833Z","iopub.status.idle":"2024-04-20T14:37:56.420561Z","shell.execute_reply.started":"2024-04-20T14:37:43.3708Z","shell.execute_reply":"2024-04-20T14:37:56.41971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup datasets","metadata":{}},{"cell_type":"code","source":"IMG_SIZE = 224\nHIDDEN_LAYERS = [128, 64, 32]\nUSE_HIDDEN_LAYERS = False\nUSE_AUGUMENTATION = False\nINPUT_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\nBATCH_SIZE = 16\nEPOCHS = 100","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:37:56.42216Z","iopub.execute_input":"2024-04-20T14:37:56.422694Z","iopub.status.idle":"2024-04-20T14:37:56.427832Z","shell.execute_reply.started":"2024-04-20T14:37:56.422668Z","shell.execute_reply":"2024-04-20T14:37:56.426749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_name = \"busi\"\ndataset_paths = [\n    '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT',\n    '/kaggle/input/busi-dcgan-001/breast-cancer-ultrasound-dcgan-generative'\n]","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:37:56.429074Z","iopub.execute_input":"2024-04-20T14:37:56.4294Z","iopub.status.idle":"2024-04-20T14:37:56.465485Z","shell.execute_reply.started":"2024-04-20T14:37:56.429368Z","shell.execute_reply":"2024-04-20T14:37:56.464707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EXPERIMENTS_CLASSES = '__all__'","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:37:56.467958Z","iopub.execute_input":"2024-04-20T14:37:56.468253Z","iopub.status.idle":"2024-04-20T14:37:56.474325Z","shell.execute_reply.started":"2024-04-20T14:37:56.468224Z","shell.execute_reply":"2024-04-20T14:37:56.473514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = 224\n\ndata_dict = defaultdict(list)\nerror_count = 0\nimage_paths = []\nimages = []\nlabels = []\nmasks = []\n\nlimit = 2000\n\nfor path in dataset_paths:\n    for dirpath, _, filenames in os.walk(path):\n        for filename in tqdm(filenames):\n            \n            if 'mask' in filename or 'h5' in filename: continue\n            label = dirpath.split('/')[-1]\n            if len(data_dict[label]) >= limit: continue\n            try:\n                path = os.path.join(dirpath, filename)\n                image = cv2.imread(path) # Check for no error\n                image = cv2.resize(image, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                \n                data_dict[label].append(image)\n                image_paths.append(path)\n                images.append(image)\n                labels.append(label)\n            except:\n#                 print('ERROR: ', filename)\n                error_count += 1\n\nprint(\"ErrorCount:\", error_count)\nprint(\"Total Images:\", len(images))","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:37:56.475462Z","iopub.execute_input":"2024-04-20T14:37:56.476007Z","iopub.status.idle":"2024-04-20T14:38:52.299578Z","shell.execute_reply.started":"2024-04-20T14:37:56.475975Z","shell.execute_reply":"2024-04-20T14:38:52.29869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"{ k: len(v) for k, v in data_dict.items() }","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:52.301046Z","iopub.execute_input":"2024-04-20T14:38:52.301361Z","iopub.status.idle":"2024-04-20T14:38:52.308712Z","shell.execute_reply.started":"2024-04-20T14:38:52.301335Z","shell.execute_reply":"2024-04-20T14:38:52.307829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(9, 3))\nsns.barplot(x=list(map(len, data_dict.values())), y=list(data_dict.keys()))\nplt.title(\"\\nThe Count of images in each labels\\n\", weight=\"bold\")","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:52.309801Z","iopub.execute_input":"2024-04-20T14:38:52.31024Z","iopub.status.idle":"2024-04-20T14:38:52.662064Z","shell.execute_reply.started":"2024-04-20T14:38:52.310206Z","shell.execute_reply":"2024-04-20T14:38:52.66126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nplt.rcParams.update({'font.size': 14})\n\ndef visualize_datasets(images, labels, k=4, cols=4, seed=42):\n    # visualize datasets\n\n    if k > len(images): k = len(images)\n    fig = plt.figure(figsize=(5 * cols, 3))\n    rows = math.ceil(k / cols)\n\n    for i in range(k):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        ax.title.set_text(labels[i])\n        plt.imshow(images[i] / 255)\n        plt.colorbar()\n        plt.axis('off')\n\n    plt.show()\n\nsample_images = [data_dict[k][1] for k in data_dict]\nvisualize_datasets(sample_images, list(data_dict.keys()), k=len(data_dict), cols=3)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:52.663499Z","iopub.execute_input":"2024-04-20T14:38:52.664246Z","iopub.status.idle":"2024-04-20T14:38:53.38254Z","shell.execute_reply.started":"2024-04-20T14:38:52.664212Z","shell.execute_reply":"2024-04-20T14:38:53.381624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data augumentation","metadata":{}},{"cell_type":"code","source":"img_augmentation_layers = [\n    layers.RandomRotation(factor=0.8),\n    layers.RandomFlip(),\n]\n\ndef img_augmentation(images, k=1):\n    \n    results = []\n    for i in range(k):\n        x = images\n        for layer in img_augmentation_layers:\n            x = layer(x)\n        results.extend(x)\n        \n    return results","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:53.383796Z","iopub.execute_input":"2024-04-20T14:38:53.38416Z","iopub.status.idle":"2024-04-20T14:38:53.410606Z","shell.execute_reply.started":"2024-04-20T14:38:53.384127Z","shell.execute_reply":"2024-04-20T14:38:53.409892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_per_class = 1350\nall_images = [*images]\nall_labels = [*labels]\n\nif USE_AUGUMENTATION:\n    aug_dict = defaultdict(list)\n    for label, _images in tqdm(data_dict.items()):\n        k = int(sample_per_class / len(_images))\n        if k > 0:\n            aug_dict[label] = img_augmentation(_images, k)\n\n    for label, items in aug_dict.items():\n        size = len(items)\n        aug_labels = [label for i in range(size)]\n        all_images.extend(items)\n        all_labels.extend(aug_labels)\n\n    print(len(all_images), len(all_labels))\nelse:\n    print(\"No use data augmentation.\")","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:53.414146Z","iopub.execute_input":"2024-04-20T14:38:53.414415Z","iopub.status.idle":"2024-04-20T14:38:53.421827Z","shell.execute_reply.started":"2024-04-20T14:38:53.414391Z","shell.execute_reply":"2024-04-20T14:38:53.420903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n\nplt.figure(figsize=(9, 3))\ncounter = Counter(all_labels)\nsns.barplot(y=list(counter.keys()), x=list(counter.values()))\nplt.title(\"\\nThe Count of images in each labels after augmentation\\n\", weight=\"bold\")","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:53.42294Z","iopub.execute_input":"2024-04-20T14:38:53.423179Z","iopub.status.idle":"2024-04-20T14:38:53.711239Z","shell.execute_reply.started":"2024-04-20T14:38:53.423158Z","shell.execute_reply":"2024-04-20T14:38:53.710269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encode datasets","metadata":{}},{"cell_type":"code","source":"images = all_images\nlabels = all_labels\nclasses = list(data_dict.keys())\nnum_classes = len(classes)\nclasses","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:53.712342Z","iopub.execute_input":"2024-04-20T14:38:53.712628Z","iopub.status.idle":"2024-04-20T14:38:53.718661Z","shell.execute_reply.started":"2024-04-20T14:38:53.712604Z","shell.execute_reply":"2024-04-20T14:38:53.717825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert labels to numpy array\nx = np.stack(images, axis=0)\ny = np.array([classes.index(label) for label in labels])\n\nx.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:53.719691Z","iopub.execute_input":"2024-04-20T14:38:53.719962Z","iopub.status.idle":"2024-04-20T14:38:53.908162Z","shell.execute_reply.started":"2024-04-20T14:38:53.71994Z","shell.execute_reply":"2024-04-20T14:38:53.907249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split datasets","metadata":{}},{"cell_type":"code","source":"# Split the data into training and remaining sets (validation + test)\nx_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n# Split the remaining data into validation and test sets\nx_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n\n[\n    x_train.shape, \n    x_val.shape, \n    x_test.shape, \n    y_train.shape , \n    y_val.shape , \n    y_test.shape\n]","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:53.909363Z","iopub.execute_input":"2024-04-20T14:38:53.90972Z","iopub.status.idle":"2024-04-20T14:38:54.131962Z","shell.execute_reply.started":"2024-04-20T14:38:53.909689Z","shell.execute_reply":"2024-04-20T14:38:54.131024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper functions","metadata":{}},{"cell_type":"code","source":"def plot_acc(model_history, name):\n    plt.rcParams.update({'font.size': 14})\n    print('\\n\\n')\n    epochs = len(model_history.history[\"accuracy\"])\n    plt.figure(figsize=(12,8))\n    plt.plot(np.arange(0, epochs), model_history.history[\"accuracy\"], label=\"train_acc\")\n    plt.plot(np.arange(0, epochs), model_history.history[\"val_accuracy\"], label=\"val_acc\")\n    plt.title(\"Training Accuracy - {}\".format(name))\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:54.13305Z","iopub.execute_input":"2024-04-20T14:38:54.133329Z","iopub.status.idle":"2024-04-20T14:38:54.140106Z","shell.execute_reply.started":"2024-04-20T14:38:54.133306Z","shell.execute_reply":"2024-04-20T14:38:54.139174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_loss(model_history, name):\n    plt.rcParams.update({'font.size': 14})\n    print('\\n\\n')\n    epochs = len(model_history.history[\"loss\"])\n    plt.figure(figsize=(12,8))\n    plt.plot(np.arange(0, epochs), model_history.history[\"loss\"], label=\"train_loss\", )\n    plt.plot(np.arange(0, epochs), model_history.history[\"val_loss\"], label=\"val_loss\")\n    plt.title(\"Training Loss - {}\".format(name))\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:54.141433Z","iopub.execute_input":"2024-04-20T14:38:54.14177Z","iopub.status.idle":"2024-04-20T14:38:54.152985Z","shell.execute_reply.started":"2024-04-20T14:38:54.141743Z","shell.execute_reply":"2024-04-20T14:38:54.152044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to plot confusion matrix\ndef plot_confusion_matrix(cm, classes,\n                          normalize=True,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.rcParams.update({'font.size': 14})\n    \n    # plot the confusion matrix\n    class_count = len(classes)\n    if normalize:\n        cm = np.round(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], 2)\n        \n    plt.figure(figsize=(12, 8))\n    sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n    plt.xticks(np.arange(class_count)+.5, classes, rotation=90)\n    plt.yticks(np.arange(class_count)+.5, classes, rotation=0)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(title)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:54.15417Z","iopub.execute_input":"2024-04-20T14:38:54.154453Z","iopub.status.idle":"2024-04-20T14:38:54.163263Z","shell.execute_reply.started":"2024-04-20T14:38:54.154431Z","shell.execute_reply":"2024-04-20T14:38:54.16229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, x, y):\n    scores = model.evaluate(x, y, verbose=1)\n    return scores","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:54.164243Z","iopub.execute_input":"2024-04-20T14:38:54.164497Z","iopub.status.idle":"2024-04-20T14:38:54.172228Z","shell.execute_reply.started":"2024-04-20T14:38:54.164475Z","shell.execute_reply":"2024-04-20T14:38:54.171327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_prob(model):\n    return model.predict(x_test, batch_size=BATCH_SIZE, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:54.173168Z","iopub.execute_input":"2024-04-20T14:38:54.173432Z","iopub.status.idle":"2024-04-20T14:38:54.181129Z","shell.execute_reply.started":"2024-04-20T14:38:54.1734Z","shell.execute_reply":"2024-04-20T14:38:54.180201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model):\n    predictions = predict_prob(model)\n    return np.argmax(predictions, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:54.182412Z","iopub.execute_input":"2024-04-20T14:38:54.182839Z","iopub.status.idle":"2024-04-20T14:38:54.192954Z","shell.execute_reply.started":"2024-04-20T14:38:54.182815Z","shell.execute_reply":"2024-04-20T14:38:54.192012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_metrics(y_true, y_pred):\n    \n    print(\"Visualize: y_true, y_pred top 20\")\n    print('Y_true', [i for i in y_true[:20]])\n    print('Y_pred', [j for j in y_pred[:20]])\n\n    # precision tp / (tp + fp)\n    precision = precision_score(y_true, y_pred, average='weighted')\n    print(\"Precision: {}\".format(precision))\n\n    # recall: tp / (tp + fn)\n    recall = recall_score(y_true, y_pred, average='weighted')\n    print(\"Recall:    {}\".format(recall))\n\n    # f1: 2 tp / (2 tp + fp + fn)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    print(\"F1:        {}\".format(f1))","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:54.194093Z","iopub.execute_input":"2024-04-20T14:38:54.194374Z","iopub.status.idle":"2024-04-20T14:38:54.202574Z","shell.execute_reply.started":"2024-04-20T14:38:54.194346Z","shell.execute_reply":"2024-04-20T14:38:54.201503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup Transfer Learning","metadata":{}},{"cell_type":"code","source":"early_stop = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=25,\n    restore_best_weights=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:54.20386Z","iopub.execute_input":"2024-04-20T14:38:54.204635Z","iopub.status.idle":"2024-04-20T14:38:54.211804Z","shell.execute_reply.started":"2024-04-20T14:38:54.204609Z","shell.execute_reply":"2024-04-20T14:38:54.210811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transfer_learning(model, name):\n    \n    best_weights_ph1 = f\"{dataset_name}_{name}_ph1_weights.keras\"\n    \n    callbacks_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        filepath = best_weights_ph1,\n        monitor = \"val_accuracy\",\n        mode = \"max\",\n        save_weights_only=True,\n        save_best_only = True,\n        verbose=1, # Logging when callback running\n    )\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    history = model.fit(\n        x_train,\n        y_train,\n        batch_size=BATCH_SIZE,\n        validation_data=(x_val, y_val),\n        validation_batch_size=BATCH_SIZE,\n        epochs = EPOCHS,\n        callbacks = [callbacks_checkpoint, early_stop]\n    )\n    \n    acc_max = max(history.history[\"accuracy\"])\n    acc_min = min(history.history[\"accuracy\"])\n    print(\"Training Acc:\", [acc_min, acc_max])\n    \n    val_acc_max = max(history.history[\"val_accuracy\"])\n    val_acc_min = min(history.history[\"val_accuracy\"])\n    print(\"Validation Acc:\", [val_acc_min, val_acc_max])\n    \n    best_idx = np.argmax(history.history[\"val_accuracy\"])\n    print('The best val_acc result expected at epoch {} with metrics: '.format(best_idx + 1))\n    \n    for k, vals in history.history.items():\n        print('{}: {}'.format(k, vals[best_idx]))\n    \n    print('\\nRestoring best weights and predicting validation set.')\n    model.load_weights(best_weights_ph1)\n    model.save(f\"{dataset_name}_{name}_ph1_model.keras\")\n    model.save(f\"{dataset_name}_{name}_ph1_model.h5\")\n    \n    loss, acc = evaluate(model, x_test, y_test)\n    print('Transfer Learning test scores (loss, acc):', [loss, acc])\n    plot_acc(history, f\"\\n Transfer Learning - ACC: {name} PhA.\")\n    plot_loss(history, f\"\\n Transfer Learning - LOSS: {name} PhA.\")\n    y_pred = predict(model)\n    return history, model, val_acc_max, y_pred","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:54.213288Z","iopub.execute_input":"2024-04-20T14:38:54.213983Z","iopub.status.idle":"2024-04-20T14:38:54.22613Z","shell.execute_reply.started":"2024-04-20T14:38:54.213955Z","shell.execute_reply":"2024-04-20T14:38:54.225089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup fine tuning","metadata":{}},{"cell_type":"code","source":"def fine_turning(model, name, acc_ph1):\n    \n    best_weights_ph2 = f\"{dataset_name}_{name}_ph2_weights.keras\"\n    callbacks_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        filepath = best_weights_ph2,\n        monitor = \"val_accuracy\",\n        mode = \"max\",\n        save_weights_only=True,\n        save_best_only = True,\n        verbose=1, # Logging when callback running\n    )\n    \n    for layer in model.layers:\n        if isinstance(layer, layers.BatchNormalization) or isinstance(layer, layers.LayerNormalization):\n            layer.trainable = False\n        else:\n            layer.trainable = True\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    history = model.fit(\n        x_train, \n        y_train,\n        batch_size=BATCH_SIZE,\n        validation_data=(x_val, y_val),\n        validation_batch_size=BATCH_SIZE,\n        epochs = EPOCHS,\n        callbacks = [callbacks_checkpoint, early_stop]\n    )\n    \n    acc_max = max(history.history[\"accuracy\"])\n    acc_min = min(history.history[\"accuracy\"])\n    print(\"Training Acc:\", [acc_min, acc_max])\n    \n    val_acc_max = max(history.history[\"val_accuracy\"])\n    val_acc_min = min(history.history[\"val_accuracy\"])\n    print(\"Validation Acc:\", [val_acc_min, val_acc_max])\n    \n    best_idx = np.argmax(history.history[\"val_accuracy\"])\n    print('The best val_acc result expected at epoch {} with metrics: '.format(best_idx))\n    for k, vals in history.history.items():\n        print('{}: {}'.format(k, vals[best_idx]))\n    \n    print('Restoring best weights of Ph2 and predicting test set.')\n    model.load_weights(best_weights_ph2)\n    model.save(f\"{dataset_name}_{name}_ph2_model.keras\")\n    model.save(f\"{dataset_name}_{name}_ph2_model.h5\")\n    \n    loss, acc = evaluate(model, x_test, y_test)\n    print('Fine Tuning test scores (loss, acc):', [loss, acc])\n    \n    if val_acc_max < acc_ph1:\n        print('\\nPhase 2 resulted in lower accuracy than Phase 1.')\n    \n    plot_acc(history, f\"\\n Fine Tuning - ACC: {name} PhB.\")\n    plot_loss(history, f\"\\n Fine Tuning - LOSS: {name} PhB.\")\n    \n    y_pred = predict(model)\n    return history, model, val_acc_max, y_pred","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:54.227475Z","iopub.execute_input":"2024-04-20T14:38:54.227802Z","iopub.status.idle":"2024-04-20T14:38:54.2423Z","shell.execute_reply.started":"2024-04-20T14:38:54.227778Z","shell.execute_reply":"2024-04-20T14:38:54.241218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Model","metadata":{}},{"cell_type":"code","source":"initial_models = dict(\n#     EfficientNetV2B3=tf.keras.applications.efficientnet_v2.EfficientNetV2B3,\n#     ResNet50V2=tf.keras.applications.resnet_v2.ResNet50V2,\n#     MobileNetV2=tf.keras.applications.mobilenet_v2.MobileNetV2,\n    EfficientNetB3=tf.keras.applications.EfficientNetB3,\n#     ResNet50=tf.keras.applications.resnet50.ResNet50,\n#     MobileNet=tf.keras.applications.mobilenet.MobileNet,\n#     MLPMixer=keras_mlp.MLPMixer,\n#     MLPMixerS32=keras_mlp.MLPMixerS32,\n#     MLPMixerS16=keras_mlp.MLPMixerS16,\n#     MLPMixerB32=keras_mlp.MLPMixerB32,\n#     MLPMixerB16=keras_mlp.MLPMixerB16,\n#     DenseNet169=tf.keras.applications.DenseNet169\n#     Xception=tf.keras.applications.Xception\n#     vit_b16=vit.vit_b16,\n#     vit_b32=vit.vit_b32\n)\n\nbase_model_kwargs = dict(\n    include_top=False,\n    weights='imagenet',\n    input_shape=INPUT_SHAPE,\n    pooling=None\n)\n\n# custom kwargs for each model here \ninitial_models_kwargs = dict(\n    EfficientNetV2B3={ **base_model_kwargs },\n    ResNet50V2={ **base_model_kwargs },\n    MobileNetV2={ **base_model_kwargs },\n    EfficientNetB3={ **base_model_kwargs },\n    ResNet50={ **base_model_kwargs },\n    MobileNet={ **base_model_kwargs },\n    Xception={ **base_model_kwargs },\n    DenseNet169={ **base_model_kwargs },\n    MLPMixer=dict(\n        pretrained=\"imagenet\",\n        num_blocks=4,\n        patch_size=14,\n        stem_width=128,\n        tokens_mlp_dim=256,\n        channels_mlp_dim=512,\n    ),\n    MLPMixerS32=dict(\n        pretrained=\"imagenet\",\n        num_classes=0\n    ),\n    MLPMixerS16=dict(\n        pretrained=\"imagenet\",\n        num_classes=0\n    ),\n    MLPMixerB32=dict(\n        pretrained=\"imagenet\",\n        num_classes=0\n    ),\n    MLPMixerB16=dict(\n        pretrained=\"imagenet\",\n        num_classes=0\n    ),\n    vit_b16=dict(\n        classes=0,\n        include_top=False,\n        pretrained_top=False,\n        pretrained=True\n    ),\n    vit_b32=dict(\n        classes=0,\n        include_top=False,\n        pretrained_top=False,\n        pretrained=True\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:54.243919Z","iopub.execute_input":"2024-04-20T14:38:54.244219Z","iopub.status.idle":"2024-04-20T14:38:54.256685Z","shell.execute_reply.started":"2024-04-20T14:38:54.244195Z","shell.execute_reply":"2024-04-20T14:38:54.255667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explanation setup","metadata":{}},{"cell_type":"code","source":"# Adapted with some modification from https://www.pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/\nclass GradCAM:\n    \n    def __init__(self, model, layerName=None):\n        self.model = model\n        self.layerName = layerName\n            \n        if self.layerName == None:\n            self.layerName = self.find_target_layer()\n    \n    def find_target_layer(self):\n        for layer in reversed(self.model.layers):\n            if len(layer.output_shape) == 4:\n                return layer.name\n        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM\")\n            \n    def compute_heatmap(self, image, classIdx, upsample_size, eps=1e-5):\n        \n        gradModel = tf.keras.Model(\n            inputs = [self.model.inputs],\n            outputs = [self.model.get_layer(self.layerName).output, self.model.output],\n            name='gradcam'\n        )\n        \n        # record operations for automatic differentiation\n        with tf.GradientTape() as tape:\n            inputs = tf.cast(image, tf.float32)\n            (convOuts, preds) = gradModel(inputs)\n            loss = preds[:, classIdx]\n        \n        # compute gradients with automatic differentiation\n        grads = tape.gradient(loss, convOuts)\n        \n        # discard batch\n        convOuts = convOuts[0]\n        grads = grads[0]\n        norm_grads = tf.divide(grads, tf.reduce_mean(tf.square(grads)) + tf.constant(eps))\n        \n        # compute weights\n        weights = tf.reduce_mean(norm_grads, axis=(0,1))\n        cam = tf.reduce_sum(tf.multiply(weights, convOuts), axis=-1)\n        \n        # Apply reLU\n        cam = np.maximum(cam, 0)\n        cam = cam / np.max(cam)\n        cam = cv2.resize(cam, upsample_size)\n        \n        # convert to 3D\n        cam3 = np.expand_dims(cam, axis=2)\n        cam3 = np.tile(cam3, [1,1,3])\n        \n        return cam3\n\n\ndef overlay_gradCAM(img, cam3):\n    cam3 = np.uint8(255 * cam3)\n    cam3 = cv2.applyColorMap(cam3, cv2.COLORMAP_JET)\n    new_img = 0.5 * cam3 + 0.5 * img\n    return (new_img * 255.0 / new_img.max()).astype(\"uint8\")\n\ndef show_gradCAMs(model, gradCAM, batch, true_label, decode_labels={}):\n    \n    plt.rcParams.update({'font.size': 13})\n    n = len(batch)\n    ncols = int(np.round(n / 2, 0))\n    plt.figure(figsize=(20, ncols * 5))\n    preds = model.predict(batch)\n    \n    for i in range(n):\n        \n        # Show original image\n        plt.subplot(ncols, 4, 2 * i + 1)\n        plt.imshow(batch[i])\n        plt.title(\"Ground Truth: {}\".format(true_label))\n        plt.axis(\"off\")\n        \n        # Show overlayed grad\n        plt.subplot(ncols, 4, 2 * i + 2)\n        pred = preds[i]\n        idx = np.argmax(preds, axis=1)[i]\n        res = [decode_labels[idx], float(np.max(pred))]\n        \n        heat_map = gradCAM.compute_heatmap(image=np.expand_dims(batch[i], axis=0), classIdx=idx, upsample_size=(IMG_SIZE, IMG_SIZE))\n        overlay_img = overlay_gradCAM(batch[i], heat_map)\n        plt.imshow(overlay_img)\n        plt.title(\"Predict: {}\\nConfidence: {:.4f}\".format(res[0], float(res[1])))\n        plt.axis(\"off\")\n        \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:54.259616Z","iopub.execute_input":"2024-04-20T14:38:54.25992Z","iopub.status.idle":"2024-04-20T14:38:54.278133Z","shell.execute_reply.started":"2024-04-20T14:38:54.259896Z","shell.execute_reply":"2024-04-20T14:38:54.277243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_gradcam(model, title):\n    print(title)\n    gradCAM = GradCAM(model=model)\n    for label, batch in data_dict.items():\n        print(f'Label: {label}.')\n        \n        batch = np.stack(batch, axis=0)[:2]\n        show_gradCAMs(model, gradCAM, batch, label, decode_labels=classes)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:54.279377Z","iopub.execute_input":"2024-04-20T14:38:54.279707Z","iopub.status.idle":"2024-04-20T14:38:54.290084Z","shell.execute_reply.started":"2024-04-20T14:38:54.279684Z","shell.execute_reply":"2024-04-20T14:38:54.289268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup save history","metadata":{}},{"cell_type":"code","source":"def save_history(history, path):\n    history_df = pd.DataFrame(data=history)\n    history_df.index.name = \"Epoch\"\n    history_df.to_csv(path)\n    print(\"Saved history completed.\")\n    print(history_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:54.294485Z","iopub.execute_input":"2024-04-20T14:38:54.295311Z","iopub.status.idle":"2024-04-20T14:38:54.300211Z","shell.execute_reply.started":"2024-04-20T14:38:54.295286Z","shell.execute_reply":"2024-04-20T14:38:54.299414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"last_models = dict()\n\nfor model_name, Model in initial_models.items():\n    \n    base_model = Model(**initial_models_kwargs[model_name])\n    base_model.trainable = False\n    \n    model_name = \"Proposed\"\n    x = base_model.inputs\n    y = base_model.layers[-1].output\n    y = layers.BatchNormalization()(y)\n    y = layers.MultiHeadAttention(num_heads=8, key_dim=512, dropout=0.1)(y, y)\n    y = layers.BatchNormalization()(y)\n    y = layers.GlobalAveragePooling2D()(y)\n    y = layers.Dense(512, activation='relu')(y)\n    output = layers.Dense(num_classes, activation='softmax')(y)\n    model = tf.keras.models.Model(x, output, name=model_name)\n    \n#     sequence_layers = [\n#         base_model,\n#         layers.GlobalMaxPooling2D(),\n#     ]\n\n#     if USE_HIDDEN_LAYERS:\n#         for i, units in enumerate(HIDDEN_LAYERS):\n#             sequence_layers.append(layers.Dense(units, activation='relu'))\n#             sequence_layers.append(layers.BatchNormalization())\n#             sequence_layers.append(layers.Dropout(0.2))\n            \n#     sequence_layers.append(layers.Dense(num_classes, activation='softmax'))\n\n#     output = sequence_layers[0].layers[-1].output\n#     for layer in sequence_layers[1:]:\n#         output = layer(output)\n    \n#     model = tf.keras.models.Model(base_model.inputs, output, name=model_name)\n    \n#     model = Sequential(sequence_layers)\n\n    print(f'\\n\\n ==========Start Process with model {model_name}=========')\n    model.summary()\n\n    history, model, best_acc_ph1, y_pred = transfer_learning(model, model_name)\n    calculate_metrics(y_test, y_pred)\n    cm = confusion_matrix(y_test, y_pred)\n    plot_confusion_matrix(cm, classes, title=f\"Confusion matrix for {model_name} - Transfer Learning\")\n    \n    save_history(history.history, f\"{model_name}-{dataset_name}-transfer-results.csv\")\n    \n    visualize_gradcam(model, \"==========GRADCAM after transfer learning==========\")\n    \n    best_acc = best_acc_ph1\n    if best_acc_ph1 < 1:\n        history, model, best_acc_ph2, y_pred = fine_turning(model, model_name, best_acc_ph1)\n        calculate_metrics(y_test, y_pred)\n        cm = confusion_matrix(y_test, y_pred)\n        plot_confusion_matrix(cm, classes, title=f\"Confusion matrix for {model_name} - Fine Turnning\")\n        save_history(history.history, f\"{model_name}-{dataset_name}-fine-tuning-results.csv\")\n        visualize_gradcam(model, \"==========GRADCAM after fine tuning==========\")\n        \n    else:\n        print('Transfer learning have 100% accuracy so no need to do fine-turning.')\n\n    print(f'==========End Process with model {model_name}==========\\n\\n')\n    \n    last_models[model_name] = model","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:54.301544Z","iopub.execute_input":"2024-04-20T14:38:54.301809Z","iopub.status.idle":"2024-04-20T15:10:37.874674Z","shell.execute_reply.started":"2024-04-20T14:38:54.301787Z","shell.execute_reply":"2024-04-20T15:10:37.873734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test and Visualize Results","metadata":{}},{"cell_type":"code","source":"k = 5\nn = len(y_test)\nsample_idx = np.random.choice(range(n), k)\nx_sample = x_test[sample_idx]\ny_sample = y_test[sample_idx]\ny_sample","metadata":{"execution":{"iopub.status.busy":"2024-04-20T15:10:37.875923Z","iopub.execute_input":"2024-04-20T15:10:37.876227Z","iopub.status.idle":"2024-04-20T15:10:37.883697Z","shell.execute_reply.started":"2024-04-20T15:10:37.876202Z","shell.execute_reply":"2024-04-20T15:10:37.882901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams.update({'font.size': 16})\n\ndef format_label(label):\n    return '\\n'.join(label.split())\n\nshort_labels = list(map(format_label, classes))\nfor model_name, model in last_models.items():\n    print(\"\\n\\nPrediction for {} model\".format(model_name))\n    y_pred = model.predict(x_sample)\n    fig, ax = plt.subplots(k, 2, figsize=(30, 25))\n    \n    for i in range(k):\n        acc = y_pred[i] * 100\n        bar_colors = ['red', 'blue', 'green', 'orange', 'purple', 'gray', 'magenta']\n        ax[i, 0].imshow(x_sample[i] / 255)\n        ax[i, 0].axis('off')\n        ax[i, 1].bar(short_labels, acc, label=short_labels, color=[bar_colors[i % len(bar_colors)] for i in range(num_classes)])\n        ax[i, 1].set_ylabel('Predict')\n        ax[i, 1].set_title('Ground Truth: {}'.format(classes[y_sample[i]]))\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T15:10:37.885077Z","iopub.execute_input":"2024-04-20T15:10:37.885423Z","iopub.status.idle":"2024-04-20T15:10:43.042205Z","shell.execute_reply.started":"2024-04-20T15:10:37.885392Z","shell.execute_reply":"2024-04-20T15:10:43.041243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model_name, model in last_models.items():\n    y_pred = np.argmax(model.predict(x_test), axis=1)\n    print(\"Classification Report for {}\".format(model_name))\n    print(classification_report(y_test, y_pred, target_names=classes))","metadata":{"execution":{"iopub.status.busy":"2024-04-20T15:10:43.043384Z","iopub.execute_input":"2024-04-20T15:10:43.043678Z","iopub.status.idle":"2024-04-20T15:10:44.238638Z","shell.execute_reply.started":"2024-04-20T15:10:43.043653Z","shell.execute_reply":"2024-04-20T15:10:44.237696Z"},"trusted":true},"execution_count":null,"outputs":[]}]}